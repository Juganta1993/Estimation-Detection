\documentclass[a4paper,english,12pt]{article}
\input{header}

%opening
\title{Lecture 18:Performance Analysis of Estimators and Cramer-Rao Lower Bound}
\date{15 Mar 2016}
\author{}
\begin{document}
\author{}
\maketitle
\section*{Continuation of last lecture}
In the last lecture, we define conjugate family for a class of pdf's or pmf's. Now look at one example for this.
\begin{exmp}(\textbf{Normal Bayes Estimators})
Let $X_1$, $X_2$,...$X_n$ be i.i.d $n(\theta , 1)$ and suppose that prior distribution on $\theta$ is $n(\mu , \tau ^2)$, (Here we assume that $\mu$ and $\tau ^2$ are both known). The posterior distribution of $\theta$ is also normal with mean and variance given by 
\begin{align*}
\mathbb{E}(\theta | x) &= \frac{\tau ^2}{\frac{1}{n} + \tau ^2} \bar{x} + \frac{\frac{1}{n}}{\frac{1}{n} + \tau ^2} \mu \\\\
Var(\theta | x) &= \frac{\frac{1}{n} \tau ^2}{\frac{1}{n} + \tau ^2}\\
\end{align*}
Note that posterior distribution belongs to the same class of distribution as that of prior distribution. Hence normal family is its own conjugate family.
\end{exmp}
\begin{rem}
Posterior mean interpolates between the prior mean and the sample mean.
\end{rem} 
\section{Performance Analysis of Estimators}
So far, we have studied different types of estimators. in this section we will study the performance of these estimators with respect to some performance criteria. Although there are several such criteria, here we will focus our attention to the mean squared error (MSE) criterion.
\subsection{Mean Squared Error Criterion}
it measures the average squared difference between the estimator $W$ and the parameter $\theta$ and is a reasonable measure of performance for a poin estimator. Moreover, it is quite tractable analytically.
\begin{defn}
The MSE of a point estimator $W$ of a parameter $\theta$ is a function of $\theta$ denoted by $MSE(W ; \theta)$ and defined by
\begin{align*}
MSE(W ; \theta) &= \mathbb{E}_\theta[(W - \theta)^2]
\end{align*}
Specifying the dependence of $W$ on $X$ expilicitly, it turns out that
\begin{align*}
MSE(W ; \theta) &= \mathbb{E}_\theta[(W(X) - \theta)^2]
\end{align*}
Applying the definition of expectation, the above equation can be written as
\begin{align*}
MSE(W ; \theta) &= \int_{\mathbb{R}^n} (W(x) - \theta)^2 f(x | \theta) dx
\end{align*} 
\end{defn}
\subsection{Bias - Variance Decomposition of MSE}
\begin{defn}
Variance of a point estimator $W$ of a parameter $\theta$ is the average value of squared difference between the estimator $W$ and its expected value $\mathbb{E}_\theta[W]$; that is 
\begin{align*}
Var_\theta[W]  = \mathbb{E}_\theta[(W - \mathbb{E}_\theta(W))^2]
\end{align*} 
\end{defn}
\begin{defn}
Bias of a point estimator $W$ of a parameter $\theta$ is the difference between the expected value of $W$ and $\theta$; that is 
\begin{align*}
Bias_\theta[W] = \mathbb{E}_\theta[W] - \theta
\end{align*}
\end{defn} 
\begin{rem}
variance of an estimator represents its variability or fluctuation\\
\end{rem}
\begin{rem}
Bias represents average accuracy of the estimator
\end{rem}
\begin{rem}
For unbiased estimators, bias is identically equal to zero and satisfies $ \mathbb{E}_\theta[W] = \theta$ for all $\theta$ in $\Theta$ 
\end{rem}
By the definition of MSE,
\begin{align*}
MSE(W ; \theta) &= \mathbb{E}_\theta[(W - \theta)^2]\\\\
&= \mathbb{E}_\theta[(W - \mathbb{E}_\theta(W) + \mathbb{E}_\theta(W) - \theta)^2]\\
&= \mathbb{E}_\theta[(W - \mathbb{E}_\theta(W))^2] + (\mathbb{E}_\theta(W) - \theta)^2\\
&= Var_\theta[W] + (Bias_\theta[W])^2
\end{align*} 
Where in the second equality, we have added and subtracted $\mathbb{E}_\theta[W]$. Third equality follows from the fact that the cross term is equal to zero and last equality follows from the definition of variance and bias of an estimator.
\begin{rem}
For unbiased estimator $MSE[W;\theta] = Var_\theta[W]$ that is MSE is equal to the variance of the unbiased estimator.
\end{rem}
Now look at some examples
\begin{exmp}(\textbf{Normal MSE})
Let $X_1$, $X_2$, ....,$X_n$ be i.i.d samples from $n(\mu , \sigma ^2)$. The statistics $\bar{X}$ and $S^2$ are both unbiased estimators since 
\begin{align*}
\mathbb{E}[\bar{X}] = \mu \ , \ \mathbb{E}[S^2] = \sigma ^2 \ for \ all \ \mu \ and \  \sigma ^2
\end{align*}
Now MSE's of these estimators are given by 
\begin{align*}
MSE(\bar{X} ; (\mu , \sigma ^2)) &= Var_{\mu , \sigma ^2}(\bar{X}) = \mathbb{E}_{\mu , \sigma ^ 2}[(\bar{X} - \mu)^2] = \frac{\sigma ^2}{n}\\
MSE(S^2 ; (\mu , \sigma ^2)) &= Var_{\mu , \sigma ^2}(S^2) = \mathbb{E}_{\mu , \sigma ^ 2}[(S^2 - \sigma ^2)^2] = \frac{2 \sigma ^4}{n - 1}
\end{align*}
An alternative estimator for $\sigma ^2$ is the maximum likelihood estimator
\begin{align*}
\hat{\sigma}^2 &= \frac{1}{n} \sum_{i=1}^{n}(X_i - \bar{X})^2 = \frac{n-1}{n} S^2\\\\
\mathbb{E}[\hat{\sigma}^2] &= \mathbb{E}[\frac{n-1}{n} S^2] = \frac{n-1}{n} \sigma ^2\\\\
Var[\hat{\sigma}^2] &= Var[\frac{n-1}{n} S^2] = \frac{2(n-1)}{n^2} \sigma ^4\\\\
Bias[\hat{\sigma}^2] &= (\mathbb{E}[\hat{\sigma}^2] - \sigma ^2)^2 =     \Big( \frac{n-1}{n} \sigma ^2 - \sigma ^2 \Big)^2 = \frac{\sigma ^4}{n^2}\\
\end{align*} 
Therefore
\begin{align*}
MSE(\hat{\sigma}^2 ; (\mu , \sigma ^2)) = \frac{2(n-1)}{n^2} \sigma ^4 + \frac{\sigma ^4}{n^2} = \frac{(2n-1)}{n^2} \sigma ^4 < \frac{2 \sigma ^4}{n-1}
\end{align*}
Whic shows that $\hat{\sigma}^2$ has smaller MSE than $S^2$ 
\end{exmp}
\begin{rem}
Unbiased estimator are natural but not necessarily MSE optimal.
\end{rem}
\section{Best Unbiased Estimators(BUE)}
In the previous section we saw that there is no one "best MSE" estimator. The reason is that the class of all estimators is too large a class. One way of finding a "best estimator" is to limit the class of estimators. So here we will consider the class of unbiased estimators only.
\begin{defn}
An estimator $W^*$ is a best unbiased estimator of $g(\theta)$ if it satisfies 
\begin{enumerate}
\item $\mathbb{E}_\theta[W^*] = g(\theta)$ for all $\theta$ in $\Theta$ and,
\item For any other estimator $W$ with \[ \mathbb{E}_\theta[W] = g(\theta) \  for \  all \  \theta  \ in \  \Theta \]
\end{enumerate} 
We have $Var_\theta(W^*) \leq Var_\theta(W)$ for all $\theta$ in $\Theta$.\\
$W^*$ is also called a uniform minimum variance unbiased estimator(UMVUE) of $g(\theta)$
\end{defn}
\begin{thm}(\textbf{Cramer - Rao Lower Bound})
It is one of the metods of finding best unbiased estimators. Let $X_1$, $X_2$, ....,$X_n$ be a sample with pdf $f(x | \theta)$ and let $W(X) = W(X_1, X_2,....,X_n)$ be any estimator satisfying
\begin{enumerate}
\item $\frac{d}{d\theta}\mathbb{E}_\theta[W(X)] = \int_{\mathbb{R}^n} \frac{\partial}{\partial \theta}[W(x) f(x | \theta)] dx$ and 
\item $Var_\theta(W(X)) < \infty$ 
\end{enumerate}
Then 
\begin{align*}
Var_\theta(W(X)) \geq \frac{\Big(\frac{d}{d\theta}\mathbb{E}_\theta[W(X)] \Big)^2}{\mathbb{E}_\theta \Big(\big(\frac{\partial}{\partial \theta} \log f(X | \theta)\big)^2\Big)}
\end{align*}
\end{thm}
\begin{proof}
The proof of this theoram follows from a clever application of Cauchy - Schwarz inequality. The basic Cauchy - Schwarz inequality is given by
\begin{align*}
\sum_{i=1}^{n} a_i b_i \leq \sqrt{\sum_{i=1}^{n} a_i^2}\sqrt{\sum_{i=1}^{n} b_i^2}
\end{align*}
For two random variables $A$ and $B$, this inequality can be written as
\begin{align*}
\mathbb{E}(AB) \leq \sqrt{\mathbb{E}(A^2)}\sqrt{\mathbb{E}(B^2)}
\end{align*} 
Substitute 
\begin{align*}
A := U - \mathbb{E}(U)\\
B := V - \mathbb{E}(V)
\end{align*}
For some random variables $U$ and $V$\\
It turns out that 
\begin{align*}
COV(U,V) & \leq \sqrt{Var(U)} \sqrt{Var(V)}\\\\
[COV(U,V)]^2 & \leq Var(U) \  Var(V)\\\\
Var(U) & \geq \frac{[COV(U,V)]^2}{Var(V)}
\end{align*}
Now choose 
\begin{align*}
U &= W(X) \  and\\
V &= \frac{\partial}{\partial \theta} \log f(X | \theta)
\end{align*}
Note that
\begin{align*}
\frac{d}{d\theta}\mathbb{E}_\theta[W(X)] &= \int_{\mathbb{R}^n} W(x) \Big[\frac{\partial}{\partial \theta}f(x | \theta)\Big] dx\\\\
&= \mathbb{E}_\theta \Big[ W(X) \frac{\frac{\partial}{\partial \theta}f(X | \theta)}{f(X | \theta)} \Big]\\\\
&= \mathbb{E}_\theta \Big[ W(X) \frac{\partial}{\partial \theta}\log f(X | \theta) \Big]\\\\
\end{align*}
First equality follows from the hypothesis of CRLB. Second equality is obtained by multiplying $\frac{f(X|\theta)}{f(X|\theta)}$ in the above equality and then using the definition of expectation. Last equality follows from the property of logarithm.  
Put $W(X) = 1$ in the above equation, it turns out that
\begin{align*}
\mathbb{E}_\theta \Big[\frac{\partial}{\partial \theta}\log f(X | \theta) \Big] = \frac{d}{d\theta} \mathbb{E}_\theta[1] = 0
\end{align*}
Therefore,
$\mathbb{E}_\theta \Big[ W(X) \frac{\partial}{\partial \theta}\log f(X | \theta) \Big]$ represents covariance between $W(X)$ and $\frac{\partial}{\partial \theta}\log f(X | \theta)$ that is 
\begin{align*}
COV_\theta \Big[ W(X), \frac{\partial}{\partial \theta}\log f(X | \theta) \Big] = \mathbb{E}_\theta \Big[ W(X) \frac{\partial}{\partial \theta}\log f(X | \theta) \Big] = \frac{d}{d\theta}\mathbb{E}_\theta[W(X)]
\end{align*}
Now 
\begin{align*}
Var_\theta \Big[\frac{\partial}{\partial \theta}\log f(X | \theta) \Big] = \mathbb{E}_\theta \Big(\frac{\partial}{\partial \theta}\log f(X | \theta) \Big)^2
\end{align*}
Using the Cauchy - Schwarz inequality together with last two equalities, we obtain
\begin{align*}
Var_\theta(W(X)) \geq \frac{\Big(\frac{d}{d\theta}\mathbb{E}_\theta[W(X)] \Big)^2}{\mathbb{E}_\theta \Big(\big(\frac{\partial}{\partial \theta} \log f(X | \theta)\big)^2\Big)}
\end{align*}

This completes the proof.
\end{proof}
   
\end{document}