\documentclass[a4paper,english,12pt]{article}
\input{header}

%opening
\title{Lecture 3: Minimax \& Neyman Pearson Hypothesis Testing}
\author{}

\begin{document}
\maketitle
In this lecture we complete the theoretical portion of our discussion of Minimax hypothesis testing rule and begin with the Neyman-Pearson hypothesis testing approach.
\section{Minimax Hypothesis Testing (Contd. from Lecture 2)}
We have already looked at the Minimax Hypothesis testing approach where the function $V(\pi_0)$ is continuous at the point of maxima. This may not be the case in many problems of interest. We therefore consider the case where $V$ has an interior maximum (i.e. maxima  $\pi_{L} \in (0,1)$) but the derivative does not exist at this point. The situation is shown in Fig. \ref{fig:DecisionRules}.
Since the derivative is not unique, there is a concern as to which decision rule $\delta_{\pi_{L}}$ should be chosen.
\par We begin by defining two decision rules as limits to the maxima from the right and left, 
\begin{eqnarray}
&\delta_{\pi_{L}}^{-}& = \underset{\pi_0 \uparrow \pi_L}{lim} \delta_{\pi_0} = \underset{\pi_0 \uparrow \pi_L}{lim} \mathbbm{1}_{\{L(y) > \tau (\pi_0)\}}, \nonumber \\
&\delta_{\pi_{L}}^{+}& = \underset{\pi_0 \downarrow \pi_L}{lim} \delta_{\pi_0},
\end{eqnarray}
where, $\delta_{\pi_0}$ is the indicator function corresponding to the likelihood function being greater than the threshold. The critical regions $\Gamma_{1}^{-}$, $\Gamma_{1}^{+}$ corresponding to the decision rules $\delta_{\pi_{L}}^{-}$, and $\delta_{\pi_{L}}^{+}$ respectively, are given by,
\begin{eqnarray}
\Gamma_{1}^{-} = \underset{\pi_0 \uparrow \pi_L}{\bigcap} \{ L(y) \geq \tau(\pi_0)\} = \{ y \in \Gamma | L(y) \geq \tau(\pi_L) \}, \nonumber \\
\Gamma_{1}^{+} = \underset{\pi_0 \downarrow \pi_L}{\bigcup} \{ L(y) > \tau(\pi_0)\} = \{ y \in \Gamma | L(y) > \tau (\pi_L) \}.
\end{eqnarray}
Let us now consider a randomized decision rule ${{\tilde \delta}_{\pi_L}}$ that uses $\Gamma_1^{-}$ with probability $q$ (where $q \in [0,1]$) and $\Gamma_1^{+}$ with probability $1-q$, at the point $L(y) = \tau(\pi_L)$. Since we know the optimum decision rule at all other points except at the point of discontinuity, at which we now choose between the limiting decision rules using randomization. This equivalent to throwing a biased coin with probability $q$ and selection a decision rule based on the outcome. We have,
\begin{equation}
{{\tilde \delta}_{\pi_L}}  = 
\begin{cases}
\mbox{choose}~ H_1, &\mbox{if }~y \in \Gamma_1^{+},\\
\mbox{choose}~ H_0, &\mbox{if }~y \in {(\Gamma_1^{-})}^c,\\
\mbox{choose}~ H_1 ~w.p.~ q, &\mbox{if }y\mbox{ is on boundary of}~\Gamma_1^{-}.
\end{cases}
\end{equation}
Since conditional risk depends on the boundary condition, we write it as,
\begin{equation}
R_j({{\tilde \delta}_{\pi_L}}) = qR_j(\delta_{\pi_L}^{-}) + (1-q)R_j(\delta_{\pi_L}^{+}),~~j=[0,1], 
\end{equation}
which is nothing but,
\begin{eqnarray}
R_0({{\tilde \delta}_{\pi_L}}) = qR_0(\delta_{\pi_L}^{-}) + (1-q)R_0(\delta_{\pi_L}^{+}), \nonumber \\
R_1({{\tilde \delta}_{\pi_L}}) = qR_1(\delta_{\pi_L}^{-}) + (1-q)R_1(\delta_{\pi_L}^{+}). 
\end{eqnarray}
So for the minimax rule at equality,
\begin{equation}
max\{ R_0({{\tilde \delta}_{\pi_L}}), R_1({{\tilde \delta}_{\pi_L}}) \} = R_0({{\tilde \delta}_{\pi_L}}) = R_1({{\tilde \delta}_{\pi_L}}).
\end{equation}
Therefore, the conditional risk equations for equality condition are,
\begin{equation*}
qR_0(\delta_{\pi_L}^{-}) + (1-q)R_0(\delta_{\pi_L}^{+}) = qR_1(\delta_{\pi_L}^{-}) + (1-q)R_1(\delta_{\pi_L}^{+}),
\end{equation*}
\begin{equation*}
q\{R_0(\delta_{\pi_L}^{-}) - R_0(\delta_{\pi_L}^{+})\} + R_0(\delta_{\pi_L}^{+}) = q\{R_1(\delta_{\pi_L}^{-}) - R_1(\delta_{\pi_L}^{+})\} + R_1(\delta_{\pi_L}^{+}),
\end{equation*}
\begin{equation}
R_0(\delta_{\pi_L}^{+}) - R_1(\delta_{\pi_L}^{+}) = q\{R_0(\delta_{\pi_L}^{+}) - R_1(\delta_{\pi_L}^{+}) + R_1(\delta_{\pi_L}^{-}) - R_0(\delta_{\pi_L}^{-})\}.
\end{equation}
So,
\begin{equation}
\label{qeq}
q = \frac{R_0(\delta_{\pi_L}^{+}) - R_1(\delta_{\pi_L}^{+})}{R_0(\delta_{\pi_L}^{+}) - R_1(\delta_{\pi_L}^{+}) + R_1(\delta_{\pi_L}^{-}) - R_0(\delta_{\pi_L}^{-})}.
\end{equation}
This gives us the probability with which we choose the alternate hypothesis at the boundary. We now represent the unconditional risk by $V$. Since $V$ is concave, it must have left hand and right hand derivative at $\pi_L$, which is denoted by $V^{'}(\pi_L^{-})$ and $V^{'}(\pi_L^{+})$. Now,
\begin{eqnarray}
V^{'}(\pi_L^{-}) &=& \frac{d}{d \pi_0} r(\pi_0 , \delta_{\pi_L}^{-}), \nonumber \\
&=& \frac{d}{d\pi_0}\{\pi_0 R_0(\delta_{\pi_L}^{-}) + (1-\pi_0)R_1(\delta_{\pi_L}^{-}) \}, \nonumber \\
&=& R_0(\delta_{\pi_L}^{-}) - R_1(\delta_{\pi_L}^{-}). 
\end{eqnarray}
Similarly, $V^{'}(\pi_L^{+}) = R_0(\delta_{\pi_L}^{+}) - R_1(\delta_{\pi_L}^{+})$. Hence eqn. (\ref{qeq}) can be written as
\begin{equation}
q = \frac{V^{'}(\pi_L^{+})}{V^{'}(\pi_L^{+}) - V^{'}(\pi_L^{-})}.
\end{equation}
We can analyze the importance of the above equation in the following manner. Figure \ref{fig:DecisionRules} shows the case in which $V$ is discontinuous at the point of maxima. The decision rule here is ${{\tilde \delta}_{\pi_L}}$. By varying the probability $q$ from $0$ to $1$ different slopes of the line $r(\pi_0, {{\tilde \delta}_{\pi_L}})$ can be obtained. The particular value of $q$ obtained from the equation above gives rise to the horizontal line.   
\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{Figures/Lec3_Fig1.jpg}
\caption[rdr]{Action of the randomized decision rule}
\label{fig:DecisionRules}
\end{figure}
\section{Neyman-Pearson Hypothesis Testing}
Earlier we saw two different methods, Bayesian and Minimax, for hypothesis testing. It can be noticed that in both cases the predominant assumption is the \textit{cost structure}, i.e. there is a penalty associated with detecting a particular Hypothesis. 
However, assigning a cost structure may not be possible always. Consider, the hypothesis test corresponding to the detection of enemy aircraft by a RADAR system. In this case, we would rather allow for some false detection (detecting an aircraft when none exists) at the cost of reduced probability of detection. Neyman Pearson criterion formalizes this approach of trading detection probability for false alarm probability. We begin by defining the required terminology.
\begin{defn}
For the hypothesis testing, we define the following terms,
\begin{itemize}
\item \textit{Type-I Error or False Alarm:} occurs when the hypothesis $H_1$ is detected given $H_0$ is true.
\item \textit{Type-II Error or Missed Detection:} occurs when the hypothesis $H_0$ is detected given $H_1$ is true.
\item \textit{Detection:} correct acceptance of $H_1$. 
\end{itemize}
\end{defn}
\begin{defn}
The probability of Type-I Error is termed as the false alarm probability, denoted by $P_F (\delta)$. 
\end{defn}
\begin{defn}
The probability of Type-II Error is termed as the probability of miss or miss probability, denoted by $P_M (\delta)$. Hence, the probability of detection is $P_D(\delta) = 1 - P_M(\delta)$. 
\end{defn}
\begin{defn}
The randomized decision rule $\tilde{\delta}: \Gamma \rightarrow [0,1]$ is a function for $H_0$ versus $H_1$ with the interpretation that for $y \in \Gamma$, $\tilde{\delta}$ is the conditional probability with which we accept $H_1$ given that we observe $Y=y$. 
\end{defn}
For a randomized rule $\tilde{\delta}$ the probability of false alarm is given by, 
\begin{equation}
P_F(\tilde{\delta}) = E_0\{\tilde{\delta}(Y)\} = \int_{\Gamma} \tilde{\delta}(y) p_0(y) \mu (dy),
\end{equation} 
where, $E_0$ is the expectation under hypothesis $H_0$. Similarly, detection probability of a randomized detection rule $\tilde{\delta}$ is given by,
\begin{equation}
P_D(\tilde{\delta}) = E_1\{\tilde{\delta}(Y)\} = \int_{\Gamma} \tilde{\delta}(y) p_1(y) \mu (dy),
\end{equation} 
where, $E_1$ is the expectation under hypothesis $H_1$.
\subsection{Neyman-Pearson Criterion}
The Neyman-Pearson criterion formulates the hypothesis testing problem as a constrained optimization problem in which the probability of detection is maximized subject to an upper-bound on the false alarm probability. Mathematically, the criterion is given by,
\begin{equation}
\underset{\delta}{\max}~~P_{D} (\delta)~~ \mbox{subject to}~P_F(\delta) \le \alpha,
\end{equation}
where $\alpha$ is the sufficiency level of the test. We will now state and prove the Neyman-Pearson lemma.
\begin{lem}{The Neyman-Pearson Lemma}
\par Let $\alpha > 0$, 
\begin{itemize}
\item[i] \texttt{Optimality:} Let $\tilde{\delta}$ be any decision rule satisfying $P_F(\tilde{\delta}) \le \alpha$, and let $\tilde{\delta^{'}}$ be any other decision rule of the form:
\begin{equation}
\label{decrule}
\tilde{\delta^{'}} (y) = 
\begin{cases}
1 &if~p_{1}(y) > \eta p_0 (y) \Rightarrow L(y) > \eta\\
\gamma(y) &if~L(y) = \eta\\
0 &if~L(y) < \eta,
\end{cases}
\end{equation}
where, $\eta \geq 0$ and $0 \leq \gamma(y) \leq 1$ such that $P_F(\tilde{\delta}) = \alpha$, then, $P_D(\tilde{\delta^{'}}) \geq P_D(\delta^{'})$.
\item[ii] \texttt{Existence:} For every $\alpha \in (0,1)$ there is a decision rule $\tilde{\delta}_{NP}$ of the form (\ref{decrule}) with $P(y) = \gamma_0$ (a constant), for which $P_F(\tilde{\delta}_{NP}) = \alpha$
\item[iii] \texttt{Uniqueness:} Let $\delta^{''}$, i.e. any $\alpha$-level Neyman-Pearson decision rule for $H_0$ versus $H_1$. Then $\delta^{''}$ must be of the form (\ref{decrule}) except possibly on a subset of $\Gamma$ having zero probability under $H_0$ and $H_1$.
\end{itemize}
\end{lem}
\begin{proof}{}
\begin{itemize}
\item[(i).] For any two decision rules of the form in eqn. (\ref{decrule}), the following equation holds,
\begin{equation}
[\tilde{\delta}^{'} (y) - \tilde{\delta} (y) ][p_1(y) - \eta p_0(y)] \geq 0, ~~~~ \forall y\in \Gamma.
\end{equation}
We have,
\begin{equation}
\int_{\Gamma} [\tilde{\delta}^{'} (y) - \tilde{\delta} (y) ][p_1(y) - \eta p_0(y)]  \mu (dy) \geq 0, 
\end{equation}
\begin{multline}
\int_{\Gamma} \tilde{\delta}^{'} (y) p_1(y)  \mu (dy) - \int_{\Gamma} \eta \tilde{\delta}^{'} (y) p_0(y)  \mu (dy) \nonumber \\
- \int_{\Gamma} \tilde{\delta} (y) p_1(y)  \mu (dy) + \int_{\Gamma} \eta \tilde{\delta} (y) p_0(y)  \mu (dy) \geq 0,
\end{multline}
\begin{multline}
\int_{\Gamma} \tilde{\delta}^{'} (y) p_1(y)  \mu (dy) - \int_{\Gamma} \tilde{\delta} (y) p_1(y)  \mu (dy)  \\
\geq  \int_{\Gamma} \eta \tilde{\delta}^{'} (y) p_0(y)  \mu (dy)  - \int_{\Gamma} \eta \tilde{\delta} (y) p_0(y)  \mu (dy).
\end{multline}
But we know that, 
\begin{equation}
P_D(\tilde{\delta}) = \int_{\Gamma} \tilde{\delta}(y) p_1(y) \mu (dy) ~~ \mbox{and} ~~  P_F(\tilde{\delta}) = \int_{\Gamma} \tilde{\delta}(y) p_0(y) \mu (dy).
\end{equation}
Therefore, we can write the above equation as,
\begin{eqnarray}
\label{faineq}
P_D(\tilde{\delta}^{'}) - P_D(\tilde{\delta}) & \geq &  \eta [P_F(\tilde{\delta}^{'}) - P_F(\tilde{\delta})], \nonumber \\
& \geq & \eta (\alpha - P_F(\tilde{\delta})), ~~~~~ \because P_F(\tilde{\delta}^{'})  = \alpha, \nonumber \\
& \geq & 0, ~~~~~~~~~~~~~~~~~~~~~ \because P_F(\tilde{\delta})  \leq \alpha.
\end{eqnarray}
Hence,
\begin{equation}
P_D(\tilde{\delta}^{'}) \geq P_D(\tilde{\delta}).
\end{equation}
\item[(ii).] Let $\eta_0$ be the smallest number such that 
\begin{equation}
\eta_0 = \inf\{\eta \in \mathbb{R}_{+} : P_0 (\{L(y) > \eta\} ) \leq \alpha\}.
\end{equation}
The term $P_0(\{L(y) > \eta\} )$ is indicative of complimentary CDF like form which exhibits right continuity. 
\par Now, if $P_0(\{L(y) > \eta_0\}) < \alpha $, then choose,
\begin{eqnarray}
\gamma_0 &=& \frac{\alpha - P_0(\{p_1(y) > \eta_0 p_0(y)\})}{P_0(\{p_1(y) = \eta_0 p_0(y)\})}, \nonumber \\
&=& \frac{\alpha - P_0(\{L(y) > \eta_0\})}{P_0(\{L(y) = \eta_0\})}.
\end{eqnarray}
otherwise, choose $\gamma_0$ arbitrarily. Then, on defining ${{\tilde \delta}_{NP}}$ to be the decision rule of form (\ref{decrule}), with $\eta = \eta_0$ and $\gamma(y) = \gamma_0$, we have, 
\begin{eqnarray}
P_F({{\tilde\delta}_{NP}}) &=& E_0\{{{\tilde\delta}_{NP}}(Y)\},  \nonumber \\
&=& P_0(p_1(Y) > \eta_0 p_0(Y)) + \gamma_0 P_0(p_1(Y) = \eta_0 p_0(Y)), \nonumber \\
&=& \alpha.
\end{eqnarray}
Thus, we have chosen a decision rule of the form (\ref{decrule}) with $\gamma(y)$ constant and false-alarm probability $\alpha$. 
\item[(iii).] Let $\tilde{\delta^{'}}$ be of the form (\ref{decrule}) and $\tilde{\delta^{''}}$ be any other $\alpha$-level Neyman-Pearson decision rule, then,
\begin{equation}
P_D(\tilde{\delta^{''}}) = P_D(\tilde{\delta^{'}}),
\end{equation}
and therefore, by equation (\ref{faineq}),
\begin{eqnarray}
0 &\geq & \alpha - P_F(\tilde{\delta^{''}}), \nonumber \\
&\geq & 0 ~~~~~ (\mbox{from part (i)}). 
\end{eqnarray}
Hence, $P_F(\tilde{\delta^{''}}) = \alpha$. Using the relations $P_F(\tilde{\delta^{''}}) = \alpha$, $P_F(\tilde{\delta^{'}}) = \alpha$, and $P_D(\tilde{\delta^{'}})=P_D(\tilde{\delta^{''}})$, we get
\begin{align}
&\{ P_D({{\tilde\delta}^{'}}) - P_D({{\tilde\delta}^{''}})\} - \eta \{ P_F({{\tilde\delta}^{'}}) - P_F({{\tilde\delta}^{''}})\} = 0,  \nonumber \\
&\int_{\Gamma}[{{\tilde\delta}}^{'} (y) - {{\tilde\delta}}^{''} (y) ][p_1(y) - \eta p_0(y)]  \mu (dy) = 0. 
\end{align}
Since the integrand is non-negative, it is zero except possibly on a set of zero probability under $H_0$ and $H_1$. Thus ${{\tilde\delta}^{'}}$ and ${{\tilde\delta}^{''}}$ differ only on the set ${y \in \Gamma | L(y) = \eta}$ and hence both have the same form (\ref{decrule}) possibly differing only in choice of $\gamma(y)$.
\end{itemize}
\end{proof}
\end{document}